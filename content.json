{"meta":{"title":"Mr.Booms' Blog","subtitle":"Machine Learning","description":null,"author":"Zhao Qi","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-11-17T07:33:55.000Z","updated":"2018-11-17T07:33:55.917Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-11-17T07:33:47.000Z","updated":"2018-11-17T07:35:45.485Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-11-17T07:33:17.000Z","updated":"2018-11-17T07:35:21.844Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"支持向量机SVM(0):简介","slug":"svm-0-introduction","date":"2018-11-16T17:32:30.000Z","updated":"2018-11-17T08:33:23.857Z","comments":true,"path":"2018/11/17/svm-0-introduction/","link":"","permalink":"http://yoursite.com/2018/11/17/svm-0-introduction/","excerpt":"","text":"支持向量机（Support Vecoter Machines）是一种非常经典的分类模型，是由Cortes和Vapnik在1995提出的，在解决小样本、非线性及高维模式识别中表现出许多特有的优势。从历史上看，支持向量机可以看做是感知机和其他简单的线性分类器的一种延伸和推广。支持向量机在上世纪90年代到2012之间非常火热，被广泛应用于模式识别和机器学习任务中，在上学期的智能技术课上，叶老师说支持向量机和Adaboost算法就是20世纪初机器学习中的屠龙刀和倚天剑！ 但是自从2012年AlexNet诞生以来，神经网络和深度学习大放异彩，导致了SVM的研究和应用日渐没落，最直接的体现就是：计算机视觉社区的研究方式从原本的提取特征+SVM分类的两阶段变成了利用深度学习直接训练端到端的模型。但是，作为一种经典的机器学习模型，支持向量机的思想比如最大间隔分类、对偶优化、软间隔、核技巧等仍然有很高的学习价值；而且，在机器学习工作的面试中，面试官甚至会让我们去手推SVM！ 这篇文章是我对SVM专题总结的第0篇，我会从整体上简要介绍SVM的知识体系和各种知识，主要是一些结论性的东西，详细的推导请参考后续文章。 我将从以下几个方面来介绍SVM： SVM的分类原则：最大间隔分类 问题的转化：拉格朗日对偶 非线性数据：软间隔支持SVM 从线性分类器到非线性分类器：核技巧 高效的学习算法：SMO算法 1V1 1VR：多分类支持向量机 梯度下降求解SVM：Hinge Loss（合页损失） 1. SVM的分类原则：最大间隔分类我们首先介绍线性的SVM，也就是说它的模型就是简单的一条直线/超平面：$wx+b=0$，其中$w$和$b$都是模型的参数,均为向量。对于某个x,如果$wx+b&gt;0$，我们就预测它的标签为正，否则为负。 对于线性可分的二分类数据而言，也许有无数条直线/分割超平面$wx+b=0$都可以将其分开，如果使用感知机算法，那么由于其随机性，我们可能得到任意一条直线，但是这些直线是等价的吗？显然是不等价的。选择不同的直线可能产生不同的泛化误差，那我们应该如何选择一条比较好的直线？以下图为例，在于这三条直线，你觉得哪一条直线最适合？ 如果你选择第三条直线，恭喜你答对了！ 于是我们就可以引出SVM的分类原则了（也就是选择直线的准则）：距离直线最近的点到直线的距离尽可能大（这就是最大间隔的含义）。我个人觉得SVM最大的贡献或者说他最牛逼的思想，就是在线性分类中引入最大间隔分类的思想。 我们将这条分类准则用数学语言翻译，就得到如下的SVM基本优化问题（或者说原问题，对应于后面的对偶问题）： \\begin{equation} \\begin{aligned} \\min_{w,b} &\\quad \\frac{1}{2}\\Vert{w}\\Vert^2\\\\ \\mathrm{s.t.} &\\quad y_i(w \\cdot x_i + b) - 1 \\ge 0, \\space i=1,2,...,N \\\\ \\end{aligned} \\end{equation}其中$D={ (x_i,y_i),i=1,2,…,N }$是训练集。这个问题很显然是个二次规划问题，于是我们可以直接调用二次规划包进行求解。 2. 问题的转化：拉格朗日对偶虽然使用二次规划包求解上面的基本问题就能得到$w$和$b$，但是SVM一般不会直接去求解原始问题，而是使用拉格朗日对偶法从原始问题导出一个对偶问题，将问题转化为对偶问题的求解。 利用拉格朗日对偶，我们导出如下的对偶问题： \\begin{equation} \\begin{aligned} \\min_{\\alpha} \\quad & \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\alpha_i\\alpha_j y_iy_j(x_i \\cdot x_j) - \\sum_{i=1}^{N}\\alpha_i\\\\ \\mathrm{s.t.} \\quad & \\sum_{i=1}^{N}\\alpha_iy_i=0\\\\ \\quad & \\alpha_i \\ge 0,\\space i=1,2,...,N \\end{aligned} \\end{equation}其中, $\\alpha_i$是拉格朗日乘子。 我们发现，SVM的对偶问题依然是个二次规划问题，所以仍然可以使用二次规划方法进行求解，求出$\\alpha_i$后，我们利用公式可以直接得出$w$和$b$： \\begin{align} &w=\\sum_{i=1}^{N}\\alpha_i y_i x_i \\\\ &b=y^* - \\sum_{i=1}^{N}\\alpha_i y_i(x_i \\cdot x^* ) \\\\ \\end{align}其中$(x^*,y^*)$是训练集中任意一个对应的$\\alpha \\ge 0$的样本点（也就是支持向量） 3. 非线性数据：软间隔SVM上面的支持向量机是线性支持向量机，对于线性可分问题（即数据集是线性可分的），线性支持向量机的原始问题和对偶问题都是有解的；但是对于线性不可分问题（即数据集是线性不可分的），线性支持向量机的原始问题的的不等式约束条件无法同时满足，因此，上述的SVM的建模，是无法解决用于非线性问题的。我们重新建模SVM,修改硬间隔最大化为软间隔最大化，这样，我们就能将SVM扩展到线性不可分问题。 我们在原始问题中引入松弛变量$\\xi$和惩罚因子$C$，得到软间隔SVM的原始问题： \\begin{equation} \\begin{aligned} \\min_{w,b} \\quad & \\frac{1}{2}\\Vert{w}\\Vert^2 + C \\sum_{i=1}^N\\xi_i\\\\ \\mathrm{s.t.} \\quad & y_i(w \\cdot x_i + b) \\ge 1 - \\xi_i, \\space i=1,2,...,N \\\\ \\quad & \\xi_i \\ge 0 \\\\ \\end{aligned} \\end{equation}其中$D={ (x_i,y_i),i=1,2,…,N }$是训练集。 进而可以得到对偶问题为： \\begin{equation} \\begin{aligned} \\min_{\\alpha} \\quad & \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\alpha_i\\alpha_j y_iy_j(x_i \\cdot x_j) - \\sum_{i=1}^{N}\\alpha_i\\\\ \\mathrm{s.t.} \\quad & \\sum_{i=1}^{N}\\alpha_iy_i=0\\\\ \\quad & 0 \\le \\alpha_i \\le C,\\space i=1,2,...,N \\end{aligned} \\end{equation}4. 从线性分类器到非线性分类器：核技巧对于SVM的对偶问题，求解这个问题，我们并不需要$x_i$本身，我们只需要要$x_i$和$x_j$的內积即$(x_i \\cdot y_i)$即可。如果我们利用一个核函数$K(x,z)$替代这个內积$(x_i \\cdot y_i)$，我们就得到了应用了核技巧的SVM的对偶问题： \\begin{equation} \\begin{aligned} \\min_{\\alpha} \\quad & \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\alpha_i\\alpha_j y_iy_j K(x_i,x_j) - \\sum_{i=1}^{N}\\alpha_i\\\\ \\mathrm{s.t.} \\quad & \\sum_{i=1}^{N}\\alpha_iy_i=0\\\\ \\quad & 0 \\le \\alpha_i \\le C,\\space i=1,2,...,N \\end{aligned} \\end{equation}使用了核技巧后，我们无法算出$w$的值，但我们可以直接得到SVM的决策函数： f(x)=sign(\\sum_{i=1}^N\\alpha_i^* y_i K(x_i, x) + b)只要选择一个支持向量，就能轻松得到$b$。 利用核函数有什么好处呢？ 根据核函数的定义，存在某个特征映射$\\phi$，对于任意的而$x$和$z$，$K(x,z)$可表达为$\\phi(x)$和$\\phi(z)$的內积，即$K(x,z)=\\phi(x) \\cdot \\phi(z)$,如果将其带入上面的对偶问题，那么应用核函数其实就相当于先对数据集作非线性变换$\\phi$，然后再应用线性SVM。这两种方式虽然是等价的，但是由于$\\phi$极其复杂，比如说$\\phi$是无穷维的，此时$K(x,z)$仍然是非常简单的，因此核函数能大大减少计算量。 5. 高效的学习算法：SMO算法虽然上述的学习问题都可以使用二次规划求解，但是由于二次规划的时空复杂度很高，往往几千个样本的数据就不行了，所以在实际中，没有什么人会使用二次规划求解SVM，人们往往采用一种更高效的算法：序列最小最优化算法（Sequential minimal optimization，SMO）。 SMO算法是一种启发式算法，基本思路就是我们熟悉的分治法。它循环遍历整个数据集，用一些启发式准则每次选择出两个样本点$x_i$和$x_j$，并以这两个样本点对应的$\\alpha_i$和$\\alpha_j$为优化变量，其他所有的$\\alpha$保持不变，这样就将N个变量的二次规划问题，转化为了两个变量的二次规划问题： \\begin{equation} \\begin{aligned} \\min_{\\alpha_i,\\alpha_j} \\quad & \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\alpha_i\\alpha_j y_iy_j K(x_i,x_j) - \\sum_{i=1}^{N}\\alpha_i\\\\ \\mathrm{s.t.} \\quad & \\sum_{i=1}^{N}\\alpha_iy_i=0\\\\ \\quad & 0 \\le \\alpha_i \\le C,\\space i=1,2,...,N \\end{aligned} \\end{equation}或者化为(不妨假定选择的两个变量为$\\alpha_1,\\alpha_2$： \\begin{equation} \\begin{aligned} \\min_{\\alpha_1,\\alpha_2} \\quad & \\frac{1}{2}K_{11}\\alpha_1^2+K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2 \\\\ \\quad & -(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_1\\alpha_1K_{i1}+y_2\\alpha_2\\sum_{i=3}^Ny_1\\alpha_1K_{i2}\\\\ \\mathrm{s.t.} \\quad & \\alpha_1y_1+\\alpha_2y_2=-\\sum_{i=3}^{N}\\alpha_iy_i=\\zeta\\\\ \\quad & 0 \\le \\alpha_i \\le C,\\space i=1,2 \\end{aligned} \\end{equation}其中$K_{ij}=K(x_i,x_j),i,j=1,2$ 6. 1V1 1VR：多分类支持向量机7. 梯度下降求解SVM：Hinge Loss（合页损失）","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"http://yoursite.com/tags/SVM/"}]},{"title":"my_new_article","slug":"my-new-article","date":"2018-11-16T15:58:29.000Z","updated":"2018-11-16T15:58:29.400Z","comments":true,"path":"2018/11/16/my-new-article/","link":"","permalink":"http://yoursite.com/2018/11/16/my-new-article/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-11-16T11:23:08.143Z","updated":"2018-11-16T11:23:08.143Z","comments":true,"path":"2018/11/16/hello-world/","link":"","permalink":"http://yoursite.com/2018/11/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}